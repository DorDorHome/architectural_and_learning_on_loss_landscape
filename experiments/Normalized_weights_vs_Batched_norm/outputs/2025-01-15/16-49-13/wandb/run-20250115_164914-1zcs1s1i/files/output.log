Epoch: 0, progress on batches:   2%|â–Ž                     | 1/59 [00:00<00:18,  3.22it/s]
Epoch:   0%|                                                     | 0/500 [00:00<?, ?it/s]
Error executing job with overrides: []
Traceback (most recent call last):
  File "/hdda/models/loss_landscape/experiments/Normalized_weights_vs_Batched_norm/single_run.py", line 82, in main
    loss, output = learner.learn(input, label)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/hdda/models/loss_landscape/src/algos/supervised/basic_backprop.py", line 39, in learn
    loss.backward()
  File "/home/sfchan/mambaforge/envs/pytorch_latest/lib/python3.11/site-packages/torch/_tensor.py", line 487, in backward
    torch.autograd.backward(
  File "/home/sfchan/mambaforge/envs/pytorch_latest/lib/python3.11/site-packages/torch/autograd/__init__.py", line 200, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
