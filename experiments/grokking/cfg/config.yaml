defaults:
  - _self_

use_wandb: True
wandb:
  project: "grokking"

seed: 42
device: "cuda:0"
epochs: 10000
batch_size: 1024 # p*p / 25, roughly
num_workers: 2

data:
  dataset: "modular_arithmetic"
  use_torchvision: false
  data_path: '/hdda/datasets' # '/home/sfchan/dataset'  #'/hdda/datasets' 
  num_classes: 113
  train_split_ratio: 0.3

net:
  _target_: "configs.configurations.GrokkingTransformerConfig"
  type: "GrokkingTransformer_pytorch_manual_implementation"
  d_model: 128
  n_layers: 1 # As per the paper
  n_heads: 4
  vocab_size: 115 # p + 2
  max_seq_len: 4 # [x, +, y, =]

learner:
  type: "backprop"
  device: ${device}
  to_perturb: false
  perturb_scale: 0.0
  enable_cuda1_workarounds: false
  network_class: null
  momentum: 0.0
  additional_regularization: null
  lambda_orth: null
  opt: "adamw"
  step_size: 1e-3
  weight_decay: 1.0
  beta_1: 0.9
  beta_2: 0.98
  loss: "cross_entropy"

evaluation:
  use_testset: True
  eval_freq_epoch: 10

track_rank: True
layers_identifier:
  - "attn.W_q"
  - "attn.W_k"
  - "attn.W_v"
  - "mlp.0"
  - "mlp.2"

