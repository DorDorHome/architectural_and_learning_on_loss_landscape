# represent experiments config:
runs: 1
run_id: 0
seed: 42
device: 'cuda:0'
epochs: 250 #epoch per tasks
batch_size: 256 #128
test: False # for testing the model after training


# for logging and tracking the experiments
use_wandb: True
log_freq_every_n_task: 1
wandb:
  project: 'shifting_tasks_on_Imagenet'
  entity: 'continual_learning'
  tags: ['shifting_tasks', 'Imagenet']
  notes: 'basic training on Imagenet with shifting tasks'

# for plasiticity-type training (implemented for Imagnet )
project_name: 'shifting_tasks_on_Imagenet'
task_id: 0 # for setting a unique but reproducible sequence of classes
train_size_per_class: 600 # number of samples per class in training
val_size_per_class: 100 # number of samples per class in validation
num_tasks: 10000 # number of tasks to train on. Each task will be trained on num_classes_per_task
num_classes_per_task: 2
new_heads_for_new_task: True

#data to use
data:
  dataset: 'imagenet_for_plasticity' #options: 'MNIST' #'CIFAR10', to implement: 'CIFAR100'. 'ImageNet' not supported yet.
  use_torchvision: False # if using Imagnet, set to False 
  data_path: '/hdda/datasets'   # '/home/sfchan/datasets' for 243, otherwise '/hdda/datasets'   #if specified, will override the dataset variable.
  # for Imagnet, expected path is folder containing the downloaded Imagnet dataset, + '/data/classes/'
  num_classes: 1000 # should be consistent with the number of classes in the dataset


# network architecture:
net:
  type: "ConvNet_SVD" #"ConvNet_SVD", "ConvNet_norm" "vgg_custom" #options: 'ConvNet', vgg_custom, "resnet_custom"
  network_class: "conv" # 'fc' for fully connected, 'conv' for convolutional
  device: ${device}
  netparams:
    pretrained: False
    num_classes: ${num_classes_per_task}
    initialization: 'kaiming'
    conv_layer_bias: False # previously false # only for "ConvNet_norm"
    linear_layer_bias: True #for "ConvNet_norm"
    # for both ConvNet_norm and ConvNet_SVD:
    weight_correction_scale: 1 #"relu_output_correction"  # 1 or "relu_output_correction" #previously 1.0#previously 1.0 for "ConvNet_norm"
    fan_in_correction: False # True # previously false for "ConvNet_norm"
    # for ConvNet_SVD:
    SVD_only_stride_1: False #always set to False
    allow_svd_values_negative: False # experiment this with True

# learning algorithm:
learner:
  type: "backprop" # Choices: 'backprop', 'cbp' 'continual_backprop'
  
  # needed only for cbp:
  neurons_replacement_rate: 0.001
  decay_rate_utility_track: 0.9
  maturity_threshold: 100
  util_type: 'contribution'
  init: 'kaiming'
  accumulate: False
  outgoing_random: False
  
  device: ${device}
  opt:  'sgd' # Choices: 'sgd',  'adam'
  loss: 'cross_entropy' # Choices: 'cross_entropy', 'mse', for classification and regression respectively
  step_size: 0.01 # for sgd try 0.01, adam try 0.001
  beta_1: 0.95
  beta_2: 0.999
  weight_decay: 0.01
  to_perturb: false
  perturb_scale: 0.05
  momentum: 0.9

# evaluation, for plotting and saving the results of the
evaluation:
  use_test_set: True
  eval_freq_epoch: 50
  eval_metrics: ['accuracy', 'loss']
  save_dir: '/results/results_raw'
  # save_name: 'basic_training'
