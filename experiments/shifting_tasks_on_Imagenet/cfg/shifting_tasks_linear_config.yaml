# represent experiments config:
runs: 1
run_id: 0
seed: 42
device: 'cuda:1'
epochs: 250 #epoch per tasks
batch_size: 256 #128
test: False # for testing the model after training


# for logging and tracking the experiments
use_wandb: True
log_freq_every_n_task: 1
wandb:
  project: 'shifting_tasks_on_Imagenet'
  entity: 'continual_learning'
  tags: ['shifting_tasks', 'Imagenet']
  notes: 'basic training on Imagenet with shifting tasks'

# for plasiticity-type training (implemented for Imagnet )
project_name: 'shifting_tasks_on_Imagenet'
task_id: 0 # for setting a unique but reproducible sequence of classes
train_size_per_class: 600 # number of samples per class in training
val_size_per_class: 100 # number of samples per class in validation
num_tasks: 10000 # number of tasks to train on. Each task will be trained on num_classes_per_task
num_classes_per_task: 2
new_heads_for_new_task: True

#data to use
data:
  dataset: 'imagenet_for_plasticity' #options: 'MNIST' #'CIFAR10', to implement: 'CIFAR100'. 'ImageNet' not supported yet.
  use_torchvision: False # if using Imagnet, set to False 
  data_path: '/hdda/datasets'  #if specified, will override the dataset variable.
  # for Imagnet, expected path is folder containing the downloaded Imagnet dataset, + '/data/classes/'
  num_classes: 1000 # should be consistent with the number of classes in the dataset


# network architecture:
# network architecture:
net:
  type: "deep_ffnn_weight_norm_multi_channel_rescale" #  #"ffnn_normal_BN" #"deep_ffnn_weight_norm_multi_channel_rescale" #"deep_ffnn_weight_batch_norm" # "deep_ffnn_weight_norm_single_rescale" # options: "deep_ffnn_weight_batch_norm" , "deep_ffnn_weight_norm" # "deep_ffnn" #
  network_class: "fc" # 'fc' for fully connected, 'conv' for convolutional
  netparams:
    #pretrained: False
    input_size: 3072 # need to match the dataset used. # 3072 # 224*224*3 for Imagenet, 784 for MNIST, 3072 for CIFAR10
    num_features: 2000
    num_outputs: ${num_classes_per_task} #$data.num_classes} 
    num_hidden_layers: 2 # 2 is enough for MNIST
    act_type: 'relu'
    initialization: 'kaiming'
    alpha_for_EMA: 0.8 # for EMA, only applicable for network that depends on EMA on batch

# learning algorithm:
learner:
  type: 'cbp' # Choices: 'backprop', 'cbp' 'continual_backprop'
  
  # needed only for cbp:
  neurons_replacement_rate: 0.001
  decay_rate_utility_track: 0.9
  maturity_threshold: 100
  util_type: 'contribution'
  init: 'kaiming'
  accumulate: False
  outgoing_random: False
  
  device: ${device}
  opt:  'adam' # Choices: 'sgd',  'adam'
  loss: 'cross_entropy' # Choices: 'cross_entropy', 'mse', for classification and regression respectively
  step_size: 0.001 # for sgd try 0.01, adam try 
  beta_1: 0.95
  beta_2: 0.999
  weight_decay: 0.01
  to_perturb: false
  perturb_scale: 0.05
  momentum: 0.9

# evaluation, for plotting and saving the results of the
evaluation:
  use_test_set: True
  eval_freq_epoch: 50
  eval_metrics: ['accuracy', 'loss']
  save_dir: '/results/results_raw'
  # save_name: 'basic_training'
