# represent experiments config:
runs: 1
run_id: 0
seed: None
device: 'cuda:0'
epochs: 50 #epoch per task switch
batch_size: 256 
num_tasks: 4000 # number of tasks to train on. Each task will be trained on num_classes_per_task

# task switching:
task_shift_mode: "input_permutation" # "input_permutation", "output_permutation", "slow_drift_bandit", "drifting_input"
task_shift_param:
  # Common parameters for permutation-based shifts.
  # Used when task_shift_mode is 'input_permutation' or 'output_permutation'.
  permutation_seed: 42 # Seed for generating reproducible permutations.

  # --- Parameters for 'input_permutation' mode ---
  input_permutation: None

  # --- Parameters for 'output_permutation' mode ---
  output_permutation: None

  # --- Parameters for 'slow_drift_bandit' mode ---
  # This mode wraps the dataset to simulate a contextual bandit problem
  # where the true value of each class drifts over time.
  slow_drift_bandit:
    # The variance of the Gaussian noise added to the true value to generate feedback.
    variance: 0.5
    # The type of drift to apply to the true values.
    # Options: 'random_walk', 'sinusoidal', 'interpolation'
    drift_mode: 'random_walk'

    # Settings for when drift_mode is 'random_walk'
    random_walk:
      drift_std_dev: 0.01 # Standard deviation of the noise added at each step.

    # Settings for when drift_mode is 'sinusoidal'
    sinusoidal:
      amplitude: 0.5 # The magnitude of the sine wave oscillation.
      frequency: 0.1 # The frequency of the sine wave (lower is slower).

    # Settings for when drift_mode is 'interpolation'
    interpolation:
      # The number of `update_drift()` calls (e.g., epochs) to transition
      # from start_values to end_values.
      duration: 100
      # The initial set of true values for each class.
      start_values: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
      # The target set of true values for each class.
      end_values:   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

  # --- Parameters for 'drifting_input' mode ---
  # This mode applies a continuously drifting affine transformation (rotation, scale, shear)
  # to the input images.
  drifting_input:
    # The type of drift to apply to the affine transformation parameters.
    # Options: 'random_walk' (currently implemented)
    drift_type: 'random_walk'

    # Settings for when drift_type is 'random_walk'
    random_walk:
      rotation_std_dev: 2.0   # Std dev for the change in angle (degrees) per step.
      scale_std_dev: 0.01     # Std dev for the change in scale per step.
      shear_std_dev: 0.02     # Std dev for the change in shear per step.
      # To enable translation drift, you would add a 'translation_std_dev' here
      # and implement it in the `_update_affine_random_walk` method.


# debug:
debug_mode: False # should be False for real experiments

#rank tracking:
track_rank: True
track_rank_batch: "use_specified" # "last" or "use_specified" or "all"
specified_batch_size: 1000 # for "specified" 
rank_measure_freq_to_epoch: 1  #should agree with, or a multiple of evaluation.eval_freq_epoch
use_pytorch_entropy_for_effective_rank: True # if False, will use numpy for effective rank
prop_for_approx_or_l1_rank: 0.99 # for approximate rank, or 1 for l1 rank
numerical_rank_epsilon: 0.01 # for numerical rank, or 0 for l1 rank

# deal units checking:
track_dead_units: True
# note that leaky_relu and 'swish' will always return 0 for dead units
# selu always use a threshold of -1.7 for dead units
# if other measures are desired, please set threshold internally
threshold_for_non_saturating_act: 0.01 # for non-saturating activation functions, leaky relu, selu, elu, swish


# for test, computationally expensive
track_actual_rank: True

# for tracking average weight magnitude:
track_weight_magnitude: True
layers_identifier: None

# for logging and tracking the experiments
use_wandb: True
use_json: False
#log_freq_every_n_task: 1
wandb:
  project: 'input_vs_output_shift_with_rank_tracking'
  entity: ''
  # tags: ['shifting_tasks', 'Imagenet']
  # notes: 'basic training on Imagenet with shifting tasks'



#data to use
data:
  dataset: 'MNIST' 
  use_torchvision: True # if using Imagnet, set to False 
  data_path: '/hdda/datasets'    # '/home/sfchan/datasets' for 243, otherwise '/hdda/datasets'   #if specified, will override the dataset variable.
  # for Imagnet, expected path is folder containing the downloaded Imagnet dataset, + '/data/classes/'
  num_classes: 10 # should be consistent with the number of classes in the dataset




# network architecture:
net:
  # to replicate the results in the paper, "deep_ffnn"
  type: "ConvNet_FC_layer_norm" # "ConvNet_batch_norm", "ConvNet_FC_layer_norm", "ConvNet_conv_and_FC_layer_norm" "ConvNet_norm", "vgg_custom" #options: "ConvNet", "vgg_custom" , "resnet_custom" , 'full_rank_resnet_custom'

  network_class: "conv" # 'fc' for fully connected, 'conv' for convolutional
  device: ${device}
  netparams:
    pretrained: False  
    num_classes: ${data.num_classes} #$data.num_classes} 
    initialization: 'kaiming'   
    input_height: None
    input_width: None
    conv_layer_bias: True # previously false # only for "ConvNet_norm"
    linear_layer_bias: True #for "ConvNet_norm"
    activation: 'leaky_relu'  # note that dead units won't show up in leaky_relu and swish #'selu' # 'leaky_relu', 'relu', 'tanh', 'sigmoid', 'selu', 'elu', 'swish'

    norm_param:
      layer_norm:
        eps: 1e-5 # for ConvNet_FC_layer_norm, ConvNet_conv_and_FC_layer_norm
        elementwise_affine: True # controls whether to have a learnable gain and bias, for ConvNet_FC_layer_norm, ConvNet_conv_and_FC_layer_norm
  
    # for ConvNet_batch_norm:
    #input_size: 784 # need to match the dataset used. # 3072 # 224*224*3 for Imagenet, 784 for MNIST, 3072 for CIFAR10

    # for both ConvNet_norm and ConvNet_SVD:
    weight_correction_scale: 1.0 # "relu_output_correction" #"relu_output_correction"  # 1 or "relu_output_correction" #previously 1.0#previously 1.0 for "ConvNet_norm"
    fan_in_correction: False # True # previously false for "ConvNet_norm"
    # for ConvNet_SVD:
    SVD_only_stride_1: False #always set to False
    allow_svd_values_negative: True # experiment this with True

    # doesn't affect ConvNet or ConvNet_norm:
    num_hidden_layers: 1 # 2 is enough for MNIST
    
    #alpha_for_EMA: 0.8 # for EMA, only applicable for network that depends on EMA on batch
    

    
# learning algorithm:
learner:
  type: 'backprop'  # Choices: 'backprop', 'cbp',# 'continual_backprop'
  init: 'kaiming' # might not work for 'vgg_custom_norm'
  # needed only for cbp:
  # neurons_replacement_rate: 0.001
  # decay_rate_utility_track: 0.9
  # maturity_threshold: 100
  # util_type: 'contribution'
  # init: 'kaiming'
  # accumulate: False
  # outgoing_random: False

  # needed for continual backprop and cbp:
  device: ${device}
  opt:  'sgd' # Choices: 'sgd',  'adam'
  loss: 'cross_entropy' # Choices: 'cross_entropy', 'mse', for classification and regression respectively
  step_size: 0.01 # for sgd try 0.01, adam try 0.001
  beta_1: 0.95
  beta_2: 0.999
  weight_decay: 0.01
  to_perturb: false
  perturb_scale: 0.05
  momentum: 0.9

# evaluation, for plotting and saving the results of the
evaluation:
  use_testset: False
  eval_freq_epoch: 1
  eval_metrics: ['accuracy', 'loss']
  # save_dir: '/results/results_raw' # If not provided, will be created inside the experiment folder,assume root directory is the root of the experiments
  # save_name: 'basic_training'
