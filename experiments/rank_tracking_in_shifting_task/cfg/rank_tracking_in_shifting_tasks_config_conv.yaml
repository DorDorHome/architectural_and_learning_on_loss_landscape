# represent experiments config:
runs: 1
run_id: 0
seed: 42
device: 'cuda:1'
epochs: 50 #epoch per task switch
batch_size: 256 
num_tasks: 4000 # number of tasks to train on. Each task will be trained on num_classes_per_task

# debug:
debug_mode: False # should be False for real experiments

#rank tracking:
track_rank: True
track_rank_batch: "use_specified" # "last" or "use_specified" or "all"
specified_batch_size: 1000 # for "specified" 
rank_measure_freq_to_epoch: 2  #should agree with, or a multiple of evaluation.eval_freq_epoch
use_pytorch_entropy_for_effective_rank: True # if False, will use numpy for effective rank
prop_for_approx_or_l1_rank: 0.99 # for approximate rank, or 1 for l1 rank
numerical_rank_epsilon: 0.01 # for numerical rank, or 0 for l1 rank

# deal units checking:
track_dead_units: True
# note that leaky_relu and 'swish' will always return 0 for dead units
# selu always use a threshold of -1.7 for dead units
# if other measures are desired, please set threshold internally
threshold_for_non_saturating_act: 0.01 # for non-saturating activation functions, leaky relu, selu, elu, swish


# for test, computationally expensive
track_actual_rank: True

# for tracking average weight magnitude:
track_weight_magnitude: True
layers_identifier: None

# for logging and tracking the experiments
use_wandb: True
use_json: False
#log_freq_every_n_task: 1
wandb:
  project: 'basic_training_rank_tracking'
  entity: ''
  # tags: ['shifting_tasks', 'Imagenet']
  # notes: 'basic training on Imagenet with shifting tasks'



#data to use
data:
  dataset: 'MNIST' 
  use_torchvision: True # if using Imagnet, set to False 
  data_path: '/hdda/datasets'    # '/home/sfchan/datasets' for 243, otherwise '/hdda/datasets'   #if specified, will override the dataset variable.
  # for Imagnet, expected path is folder containing the downloaded Imagnet dataset, + '/data/classes/'
  num_classes: 10 # should be consistent with the number of classes in the dataset




# network architecture:
net:
  # to replicate the results in the paper, "deep_ffnn"
  type: "ConvNet" # "vgg_custom" #options: 'ConvNet', "vgg_custom" , "resnet_custom" , 'full_rank_resnet_custom'
  network_class: "conv" # 'fc' for fully connected, 'conv' for convolutional
  device: ${device}
  netparams:
    input_height: None
    input_width: None
    pretrained: False
    input_size: 784 # need to match the dataset used. # 3072 # 224*224*3 for Imagenet, 784 for MNIST, 3072 for CIFAR10
    num_features: 2000
    num_classes: ${data.num_classes} #$data.num_classes} 
    num_hidden_layers: 1 # 2 is enough for MNIST
    act_type: 'leaky_relu' # note that dead units won't show up in leaky_relu and swish #'selu' # 'leaky_relu', 'relu', 'tanh', 'sigmoid', 'selu', 'elu', 'swish'
    initialization: 'kaiming'
    #alpha_for_EMA: 0.8 # for EMA, only applicable for network that depends on EMA on batch


    
# learning algorithm:
learner:
  type: 'backprop'  # Choices: 'backprop', 'cbp',# 'continual_backprop'
  init: 'kaiming' # might not work for 'vgg_custom_norm'
  # needed only for cbp:
  # neurons_replacement_rate: 0.001
  # decay_rate_utility_track: 0.9
  # maturity_threshold: 100
  # util_type: 'contribution'
  # init: 'kaiming'
  # accumulate: False
  # outgoing_random: False

  # needed for continual backprop and cbp:
  device: ${device}
  opt:  'sgd' # Choices: 'sgd',  'adam'
  loss: 'cross_entropy' # Choices: 'cross_entropy', 'mse', for classification and regression respectively
  step_size: 0.01 # for sgd try 0.01, adam try 0.001
  beta_1: 0.95
  beta_2: 0.999
  weight_decay: 0.01
  to_perturb: false
  perturb_scale: 0.05
  momentum: 0.9

# evaluation, for plotting and saving the results of the
evaluation:
  use_testset: False
  eval_freq_epoch: 1
  eval_metrics: ['accuracy', 'loss']
  # save_dir: '/results/results_raw' # If not provided, will be created inside the experiment folder,assume root directory is the root of the experiments
  # save_name: 'basic_training'
